{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Cactus (Training)",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1NBbmGYUZbKq0fjkI2OJIJx_3PV_Rdf7Z",
      "authorship_tag": "ABX9TyM6gNml7h3d1lf9P9K8DG1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ducksss/Project-Cactus/blob/main/Project_Cactus_(Training).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41GqKSNIpLF6"
      },
      "source": [
        "<div id=\"top\"></div>\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "<!--\n",
        "*** I'm using markdown \"reference style\" links for readability.\n",
        "*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\n",
        "*** See the bottom of this document for the declaration of the reference variables\n",
        "*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\n",
        "*** https://www.markdownguide.org/basic-syntax/#reference-style-links\n",
        "-->\n",
        "[![Contributors](https://img.shields.io/github/contributors/Ducksss/Project-Cactus.svg)][contributors-url]\n",
        "[![Forks](https://img.shields.io/github/forks/Ducksss/Project-Cactus.svg)][forks-url]\n",
        "[![Stargazers](https://img.shields.io/github/stars/Ducksss/Project-Cactus.svg)][stars-url]\n",
        "[![MIT License](https://img.shields.io/github/license/Ducksss/Project-Cactus.svg)][license-url]\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NBbmGYUZbKq0fjkI2OJIJx_3PV_Rdf7Z?usp=sharing)\n",
        "\n",
        "\n",
        "<!-- PROJECT LOGO -->\n",
        "<br />\n",
        "<div align=\"center\">\n",
        "  <a href=\"https://github.com/Ducksss/Project-Cactus\">\n",
        "    <img src=\"assets/cactus-bg.png\" alt=\"Logo\" width=\"80\" height=\"80\">\n",
        "  </a>\n",
        "\n",
        "<h3 align=\"center\">Project Cactus</h3>\n",
        "\n",
        "  <p align=\"center\">\n",
        "    A cross-platform AI Fake News Detector\n",
        "    <br />\n",
        "    <br />\n",
        "    <a href=\"https://project-cactus-c9549.web.app/\">Web Application</a>\n",
        "    ·\n",
        "    <a href=\"#browser-extension\">Browser Extension</a>\n",
        "    ·\n",
        "    <a href=\"https://github.com/Ducksss/Project-Cactus/issues\">Report Bugs</a>\n",
        "    ·\n",
        "    <a href=\"https://github.com/Ducksss/Project-Cactus/issues\">Request Features</a>\n",
        "  </p>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6c5wQvSJ6k"
      },
      "source": [
        "> Note: Make sure to use a high-ram runtime, else the model may crash due to it's size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as1mESd1oM8w"
      },
      "source": [
        "## Download the Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aiWnjzYoLId"
      },
      "source": [
        "!git clone https://github.com/Ducksss/FakeNews.git "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Y9XNGYoQDj"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IuxJQ2nC2L9"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "import zipfile\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.utils import get_file\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyPHsJkMq00A"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EQoVb8goj9E"
      },
      "source": [
        "#@title Define Hyperparameters\n",
        "BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "max_words = 1000000 #@param {type:\"integer\"}\n",
        "checkpoint_path = \"/tmp/checkpoints\" #@param {type:\"string\"}\n",
        "save_dir = \"saved_model\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TJ42dy4pGyM"
      },
      "source": [
        "## Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "kdyJGvw8bUZp",
        "outputId": "c6977fa5-20d8-4342-8b0c-f171377b09ef"
      },
      "source": [
        "dataset_dir = \"data/fakeNews.csv\"\n",
        "df = pd.read_csv(dataset_dir)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>isFakeNews</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Coronavirus was created in a government lab as...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>COVID-19-rumor-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The lie that coronavirus came from a bat or a ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>COVID-19-rumor-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The health experts had predicted the virus cou...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>COVID-19-rumor-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A video clip supposedly showed that the expone...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>COVID-19-rumor-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Almost 200 people in Italy died from the coron...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>COVID-19-rumor-dataset</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                     src\n",
              "0  Coronavirus was created in a government lab as...  ...  COVID-19-rumor-dataset\n",
              "1  The lie that coronavirus came from a bat or a ...  ...  COVID-19-rumor-dataset\n",
              "2  The health experts had predicted the virus cou...  ...  COVID-19-rumor-dataset\n",
              "3  A video clip supposedly showed that the expone...  ...  COVID-19-rumor-dataset\n",
              "4  Almost 200 people in Italy died from the coron...  ...  COVID-19-rumor-dataset\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn1E20GOlyW1"
      },
      "source": [
        "max_seqlen = df[\"title\"].apply(lambda x : len(x.split())).max()\n",
        "max_seqlen"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e2Z9MZZjnVM",
        "outputId": "75024a9e-58e8-40d6-eec9-77ce7289964b"
      },
      "source": [
        "dataset_len = len(df)\n",
        "dataset_len"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78578"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ8M_t0KjrBb"
      },
      "source": [
        "def train_test_split(dataset, dataset_len, val_split=0.2, shuffle=True, shuffle_size=50000):\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(shuffle_size, seed=42)\n",
        "    train_size = int((1-val_split) * dataset_len)\n",
        "    val_size = int(val_split * dataset_len)\n",
        "    try:\n",
        "      train_ds = dataset.take(train_size).map(lambda x : (x[\"title\"], x[\"isFakeNews\"]))\n",
        "      val_ds = dataset.skip(train_size).take(val_size).map(lambda x : (x[\"title\"], x[\"isFakeNews\"]))\n",
        "    except:\n",
        "      train_ds = dataset.take(train_size)\n",
        "      val_ds = dataset.skip(train_size).take(val_size)\n",
        "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return train_ds, val_ds"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPJ60Venfdzn"
      },
      "source": [
        "ds = tf.data.experimental.make_csv_dataset(dataset_dir, select_columns=[\n",
        "    \"title\",\n",
        "    \"isFakeNews\"                                                                    \n",
        "], batch_size=BATCH_SIZE)\n",
        "\n",
        "train_ds, val_ds = train_test_split(ds, dataset_len)\n",
        "val_ds, test_ds = train_test_split(val_ds, int(dataset_len * 0.2), val_split=0.5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOw0mMMMEG3p"
      },
      "source": [
        "## CactusNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6JroLfFd63E",
        "outputId": "34f32d18-e937-41c8-b3ec-8a53a40b48f0"
      },
      "source": [
        "@tf.keras.utils.register_keras_serializable() # Decorator to allow us to save the TextVectorizer layer\n",
        "def text_preprocessor(text):\n",
        "  punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "  stopwords = {'whom', 'all', 'shouldn', 'wouldn', 'how', 's', 'they', 'were', 'mustn', 'after', 'who', 'its', 'our', 't', 'a', 'very', 'an', 'do', 'be', 'to', 'can', 'had', 'i', 'these', 'himself', 'up', 'just', 'them', 'now', 'has', 'too', 'below', 'did', 'shan', 'until', 'during', 'him', 'into', 'have', \"you'd\", 'haven', 'theirs', 'ourselves', 'once', \"isn't\", 'than', \"it's\", 'wasn', 'yours', \"mightn't\", 'here', 'ours', 'her', 'doing', 'd', 'yourself', 'y', 'before', 'does', 'then', 'between', 'some', 'with', \"needn't\", 'but', 'didn', \"shouldn't\", 'that', \"weren't\", 'which', 'or', \"hasn't\", 'own', 'about', 'what', \"aren't\", 'couldn', 'doesn', 'as', \"wouldn't\", 'hasn', 'no', 'm', 'hers', 'hadn', 'aren', 'while', 'will', \"don't\", \"shan't\", 'why', 'at', 'mightn', 'themselves', 'weren', \"that'll\", 'isn', 'only', 'the', 'been', \"couldn't\", 'don', 'should', 'same', 'both', 'where', 'was', 'me', 'through', \"hadn't\", 've', 'against', 'if', 'under', 'such', 'is', 'll', \"haven't\", 'ain', 're', \"didn't\", 'nor', 'not', 'being', 'are', 'your', 'over', 'off', 'having', 'by', \"won't\", 'myself', 'out', 'more', \"wasn't\", \"doesn't\", 'won', 'this', 'my', 'again', 'ma', 'his', 'when', 'you', 'there', 'herself', 'yourselves', 'itself', 'of', \"she's\", 'needn', 'we', \"mustn't\", 'above', \"you're\", 'so', 'it', \"should've\", 'am', 'he', 'those', 'further', 'she', 'down', 'on', \"you'll\", 'for', 'other', 'any', 'their', 'from', 'each', 'most', 'because', 'and', 'few', 'in', \"you've\", 'o'}\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.regex_replace(text, \"<[^>]+>\", \"\") # remove html tags\n",
        "  text = tf.strings.regex_replace(text, '[%s]' % punctuation, \"\") # remove punctuation\n",
        "  for stopword in stopwords:\n",
        "    text = tf.strings.regex_replace(text, r\"\\b%s\\b\" % stopword, \"\") # remove stopwards\n",
        "  \n",
        "  return text\n",
        "\n",
        "sample_text = \"<p>I'm very <span class='bold'>mad</span> about the results of the election!!! Who agrees with me?</p>\"\n",
        "print(text_preprocessor(sample_text))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'im  mad   results   election  agrees  ', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUtAoID2iE8w"
      },
      "source": [
        "def create_tokenizer(train_ds, max_words, max_seqlen, output_mode = \"int\", standardize = \"lower_and_strip_punctuation\"):\n",
        "  train_text = train_ds.map(lambda x, y : x)\n",
        "  tokenizer = tfl.TextVectorization(\n",
        "      standardize=standardize,\n",
        "      max_tokens=max_words,\n",
        "      output_sequence_length=max_seqlen,\n",
        "      output_mode=output_mode\n",
        "  )\n",
        "  tokenizer.adapt(train_text)\n",
        "  return tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysZAIpE6mkO3"
      },
      "source": [
        "tokenizer = create_tokenizer(train_ds, max_words, max_seqlen, standardize=text_preprocessor)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL8trLUGGaGi"
      },
      "source": [
        "def load_pretrained_embeddings_v1(url, output_file, embedding_file, embedding_dim, vocabulary, max_words, max_seqlen):\n",
        "  embedding_vecs = dict()\n",
        "  word_idx = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "  file_dir = get_file(output_file, url)\n",
        "\n",
        "  with zipfile.ZipFile(file_dir, \"r\") as f:\n",
        "    f.extractall(\"/content/\")\n",
        "\n",
        "  with open(embedding_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      embedding_vec = np.asarray(values[1:], dtype='float32')\n",
        "      embedding_vecs[word] = embedding_vec\n",
        "\n",
        "  embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "  \n",
        "  for word, idx in word_idx.items():\n",
        "    if idx < max_words:\n",
        "      embedding_vec = embedding_vecs.get(word)\n",
        "      if embedding_vec is not None:\n",
        "        embedding_matrix[idx] = embedding_vec\n",
        "  \n",
        "  embedding = tfl.Embedding(max_words, embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=False, input_length=max_seqlen, trainable=False)\n",
        "  return embedding"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVppQOpuncqn"
      },
      "source": [
        "vocabulary = tokenizer.get_vocabulary()\n",
        "embedding = load_pretrained_embeddings_v1(\"https://nlp.stanford.edu/data/glove.twitter.27B.zip\", \"glove.twitter.27B.zip\", \"glove.twitter.27B.100d.txt\", 100, vocabulary=vocabulary, max_words=max_words, max_seqlen=max_seqlen)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m38BME4EJIk"
      },
      "source": [
        "def create_cactusnet_v2(tokenizer, embedding_layer, max_words, max_seqlen, optimizer='adam'):\n",
        "  model = Sequential(\n",
        "      [\n",
        "      tokenizer,\n",
        "      embedding_layer,\n",
        "      tfl.Bidirectional(tfl.LSTM(128, return_sequences=True, input_shape=(max_words, max_seqlen))),\n",
        "      tfl.Bidirectional(tfl.LSTM(128, return_sequences=False)),\n",
        "      tfl.Dropout(0.2),\n",
        "      tfl.Dense(1, activation='sigmoid')\n",
        "      ]\n",
        "  )\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEXgYiq7qBs2",
        "outputId": "a690469e-ac83-43a8-897b-ec3522ff60d0"
      },
      "source": [
        "model = create_cactusnet_v2(tokenizer, embedding, max_words, max_seqlen)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_vectorization_1 (TextVe (None, 143)               0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 143, 100)          100000000 \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 143, 256)          234496    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 100,628,993\n",
            "Trainable params: 628,993\n",
            "Non-trainable params: 100,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hEwnOTGp0Bw"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyxmQHdmQ6Nk"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TerminateOnNaN, EarlyStopping\n",
        "\n",
        "callbacks = [\n",
        "             ModelCheckpoint(checkpoint_path),\n",
        "             ReduceLROnPlateau(),\n",
        "             TerminateOnNaN(),\n",
        "             EarlyStopping(patience=2)\n",
        "]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eH9AT1nTANn"
      },
      "source": [
        "def train_model(model, training_ds, validation_ds = None, val_split = 0.2, batch_size = BATCH_SIZE, epochs=5, callbacks=callbacks):\n",
        "  if validation_ds is None:\n",
        "    history = model.fit(training_ds, validation_split=val_split, batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
        "  else:\n",
        "    history = model.fit(training_ds, validation_data=validation_ds, batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
        "  return history"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg5KKonwqz96",
        "outputId": "6a83cf10-60c5-49ac-f2dc-165153dd4c8d"
      },
      "source": [
        "history = train_model(model, train_ds, val_ds, epochs=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62862/62862 [==============================] - 3053s 48ms/step - loss: 0.0545 - accuracy: 0.9781 - val_loss: 0.0103 - val_accuracy: 0.9957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Data/NTU MLDA Hackathon 2021/Model Checkpoints/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Data/NTU MLDA Hackathon 2021/Model Checkpoints/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRA4n3JzLI55",
        "outputId": "ccd3109b-89ff-4507-fe6d-e87733123268"
      },
      "source": [
        "model.evaluate(test_ds)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7857/7857 [==============================] - 162s 21ms/step - loss: 0.0103 - accuracy: 0.9957\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.010271494276821613, 0.9957382678985596]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qiul3NJ1wKl",
        "outputId": "e7859edb-4d2c-471f-aad1-2dcea6a21f91"
      },
      "source": [
        "model.save(save_dir)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Data/NTU MLDA Hackathon 2021/Modelv2_3/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Data/NTU MLDA Hackathon 2021/Modelv2_3/assets\n"
          ]
        }
      ]
    }
  ]
}